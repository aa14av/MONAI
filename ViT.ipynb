{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a60234f-55e5-40c3-96f2-9d027a386ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"/home/alexanderalbizu\")\n",
    "sys.path.append(\"/home/alexanderalbizu/.local/bin\")\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"CNN_II.ipynb\"\n",
    "#!python setup.py develop \n",
    "#!pip install wandb\n",
    "#!pip install 'monai[all]'\n",
    "#!pip -q install vit_pytorch\n",
    "#!pip -q install linformer\n",
    "#!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# !pip install git+https://github.com/shijianjian/EfficientNet-PyTorch-3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a530996e-56c1-4e5b-b154-98b8d12d8a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maalbizu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import json, glob, random, re, collections, time, cv2, torch\n",
    "\n",
    "import numpy as np\n",
    "import wandb as wb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "from scipy.io import loadmat as lm\n",
    "from enum import Enum\n",
    "from torch import nn\n",
    "from torch.utils import data as torch_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import functional as torch_functional\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from monai.networks import nets\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    ImageDataset,\n",
    "    CSVDataset,\n",
    ")\n",
    "from monai.transforms import (\n",
    "    AddChannel,\n",
    "    Compose,\n",
    "    RandGaussianNoise,\n",
    "    Resize,\n",
    "    RemoveRepeatedChannel,\n",
    "    Orientation,\n",
    "    RandRotate90,\n",
    "    RandBiasField,\n",
    "    ScaleIntensity,\n",
    "    ToDevice,\n",
    "    EnsureType,\n",
    ")\n",
    "from efficientnet_pytorch_3d import EfficientNet3D\n",
    "wb.login(); # 7e5f63e5846f29b034d98806712ab047df76834d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6bbeee4-4365-4bc7-a277-5b09c4b5f5d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 1 but corresponding boolean dimension is 349",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4871a16f2077>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mEPI_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrootDir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'func'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sub-'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'_ROIconnectivity.csv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mEPI_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPI_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDiagnosis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 1 but corresponding boolean dimension is 349"
     ]
    }
   ],
   "source": [
    "# Set data directory\n",
    "rootDir = '/blue/camctrp/working/gullettj/ACT/derivatives/SVM/';\n",
    "sdata = pd.read_csv(os.path.join(rootDir,'subjects_pre-mci_classification.csv')).to_numpy();\n",
    "mri_types = ['T1', 'EPI']\n",
    "T1_SIZE = (64,64,64)\n",
    "T1_PATCH = (16,16,16)\n",
    "EPI_SIZE = (51,51,1)\n",
    "EPI_PAT = (17,17,17)\n",
    "BATCH_SIZE = 4\n",
    "N_EPOCHS = 5\n",
    "SEED = 42\n",
    "LEARNING_RATE = 5e-4\n",
    "LR_DECAY = 0.9\n",
    "samples_to_exclude = np.array([103744,106986,300142,101644,105903,106078,106817,101395,105554,204085])\n",
    "exclude = np.equal(np.isin(sdata[:,0],samples_to_exclude),0);\n",
    "\n",
    "num_of_cores = int(os.environ['SLURM_CPUS_PER_TASK'])\n",
    "\n",
    "# Two binary labels for Healthy vs pre-MCI\n",
    "subIdx = np.arange(sdata[exclude,0].shape[0]);\n",
    "lab = np.array([sdata[:,1]]);\n",
    "lab = lab[:,exclude]; \n",
    "labels = torch.nn.functional.one_hot(torch.as_tensor(lab.T)).float()\n",
    "\n",
    "# Remove 20ch Head Coils\n",
    "T1w_images = np.array([os.path.join(rootDir,'anat',''.join(['wc0csub-',str(sdata[s,0]),'_ses-01_T1w.nii'])) for s in range(sdata.shape[0])]);\n",
    "T1w_images = T1w_images[exclude];\n",
    "\n",
    "for s in range(sdata.shape[0]):\n",
    "    EPI_images = np.array([os.path.join(rootDir,'func',''.join(['sub-',str(sdata[s,0]),'_ROIconnectivity.csv'])) ]);\n",
    "EPI_images = EPI_images[exclude];\n",
    "\n",
    "class Diagnosis(Enum):\n",
    "    normal = 0\n",
    "    preMCI = 1\n",
    "\n",
    "# Plot Labels\n",
    "plt.rcParams['figure.figsize'] = [20, 20]; plt.imshow(lab); \n",
    "plt.axis('off'); plt.title('Class Labels');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd34481-edf2-47b7-82ba-6a0a9ea4c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(SEED)    \n",
    "\n",
    "# Plot OG Images\n",
    "fig, axes = plt.subplots(*[1,2], figsize=(5, 5), facecolor=\"white\")\n",
    "\n",
    "# T1w\n",
    "axes[0].set_title(''.join(['sub-',str(sdata[0,0]),' T1 Data']));\n",
    "nii = nib.load(T1w_images[0]); # Load Each Electrode\n",
    "axes[0].imshow(nii.get_fdata()[:, :, 100].T, cmap=\"gray\", origin=\"lower\"); axes[0].axis('off'); \n",
    "# EPI\n",
    "axes[1].set_title(''.join(['sub-',str(sdata[0,0]),' CONN Data']));\n",
    "mat = pd.read_csv(EPI_images[0],delimiter=',',header=None); # Load Each Electrode\n",
    "axes[1].imshow(mat, cmap=\"turbo\"); axes[1].axis('off');\n",
    "print('T1 Shape:',nii.shape,'\\nCONN Shape:',mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e02e14-de17-4748-bb95-42e49d2a1cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        mlp_dim = 2048\n",
    "        for _ in range(depth):\n",
    "            #print (dim, mlp_dim)\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "        num_patches = ((image_size // patch_size) ** 3); # 3D\n",
    "        patch_dim = channels * patch_size ** 3\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        #print (mlp_dim)\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "        #print (dim)\n",
    "        self.to_cls_token = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, num_classes),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, mask = None):\n",
    "        p = self.patch_size # PAT_SIZE 1D\n",
    "#         print (img.shape)\n",
    "        x = rearrange(img, 'b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1 = p, p2 = p, p3 = p) # SPLIT IMAGE INTO 3D PATCHES (b, # patchs, # patch features)\n",
    "#         print (x.shape)\n",
    "        x = self.patch_to_embedding(x) # Initialize Patch Weights\n",
    "#         print (x.shape)\n",
    "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1) \n",
    "#         print (cls_tokens.shape)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "#         print (x.shape)\n",
    "#         print (self.pos_embedding.shape)\n",
    "        x += self.pos_embedding # Gradient Step\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        return self.mlp_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3953db24-55ed-46ca-8049-b74305029a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        device, \n",
    "        optimizer,\n",
    "        criterion\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=LR_DECAY)\n",
    "        self.criterion = criterion\n",
    "\n",
    "        self.best_valid_score = 0.0\n",
    "        self.n_patience = 0\n",
    "        self.lastmodel = None\n",
    "        \n",
    "        self.val_losses = []\n",
    "        self.train_losses = []\n",
    "        self.val_auc = []\n",
    "        \n",
    "        wb.init(project=\"ACT-CNN\",\n",
    "           config={\n",
    "               \"batch_size\": BATCH_SIZE,\n",
    "               \"image_size\": IM_SIZE,\n",
    "               \"learning_rate\": LEARNING_RATE,\n",
    "               \"lr_decay\": LR_DECAY,\n",
    "               \"dataset\": \"ACT\",\n",
    "           })\n",
    "        wb.watch(self.model, log='all')\n",
    "    def fit(self, epochs, train_loader, valid_loader, save_path, patience):      \n",
    "        \n",
    "        for n_epoch in range(1, epochs + 1):\n",
    "            self.info_message(\"EPOCH: {}\", n_epoch)\n",
    "            \n",
    "            train_loss, train_time = self.train_epoch(train_loader)\n",
    "            wb.log({'train_loss': train_loss, 'train_time': train_time})\n",
    "            valid_loss, valid_auc, valid_time = self.valid_epoch(valid_loader)\n",
    "            wb.log({'valid_loss': valid_loss, 'valid_auc': valid_auc, 'valid_time': valid_time})\n",
    "            \n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(valid_loss)\n",
    "            self.val_auc.append(valid_auc)\n",
    "            \n",
    "            self.info_message(\n",
    "                \"[Epoch Train: {}] loss: {:.4f}, time: {:.2f} s\",\n",
    "                n_epoch, train_loss, train_time\n",
    "            )\n",
    "            \n",
    "            \n",
    "            self.info_message(\n",
    "                \"[Epoch Valid: {}] loss: {:.4f}, auc: {:.4f}, time: {:.2f} s\",\n",
    "                n_epoch, valid_loss, valid_auc, valid_time\n",
    "            )\n",
    "\n",
    "            if self.best_valid_score < valid_auc: \n",
    "                self.save_model(n_epoch, save_path, valid_loss, valid_auc)\n",
    "                self.info_message(\n",
    "                     \"auc improved from {:.4f} to {:.4f}. Saved model to '{}'\", \n",
    "                    self.best_valid_score, valid_auc, self.lastmodel\n",
    "                )\n",
    "                self.best_valid_score = valid_auc\n",
    "                self.n_patience = 0\n",
    "            else:\n",
    "                self.n_patience += 1\n",
    "            \n",
    "            if self.n_patience >= patience:\n",
    "                self.info_message(\"\\nValid auc didn't improve last {} epochs.\", patience)\n",
    "                break\n",
    "        wb.run.log_code(\".\"); wb.finish()\n",
    "        \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        t = time.time()\n",
    "        sum_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(train_loader, 1):\n",
    "            X = batch[0].clone().float().to(self.device);\n",
    "            targets = batch[1].to(self.device);\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(X).squeeze(1)\n",
    "            loss = self.criterion(outputs, targets.float())            \n",
    "            self.scaler.scale(loss).backward()\n",
    "            \n",
    "#             loss.backward()\n",
    "\n",
    "            sum_loss += loss.detach().item()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "#             self.optimizer.step()\n",
    "            \n",
    "            message = 'Train Step {}/{}, train_loss: {:.4f}'\n",
    "            self.info_message(message, step, len(train_loader), sum_loss/step, end=\"\\r\")\n",
    "            \n",
    "        self.lr_scheduler.step()\n",
    "        \n",
    "        return sum_loss/len(train_loader), int(time.time() - t)\n",
    "    \n",
    "    def valid_epoch(self, valid_loader):\n",
    "        self.model.eval()\n",
    "        t = time.time()\n",
    "        sum_loss = 0\n",
    "        y_all = []\n",
    "        outputs_all = []\n",
    "\n",
    "        for step, batch in enumerate(valid_loader, 1):\n",
    "            with torch.no_grad():\n",
    "                targets = batch[1].to(self.device)\n",
    "\n",
    "                output = self.model(batch[0].clone().detach().float().to(self.device)).squeeze(1)\n",
    "                loss = self.criterion(output, targets.float())\n",
    "                sum_loss += loss.detach().item()\n",
    "                output = torch.sigmoid(output)\n",
    "                \n",
    "                y_all.extend(batch[1].tolist())\n",
    "                outputs_all.extend(output.tolist())\n",
    "\n",
    "            message = 'Valid Step {}/{}, valid_loss: {:.4f}'\n",
    "            self.info_message(message, step, len(valid_loader), sum_loss/step, end=\"\\r\")\n",
    "            \n",
    "        auc = roc_auc_score(y_all, outputs_all)\n",
    "        \n",
    "        return sum_loss/len(valid_loader), auc, int(time.time() - t)\n",
    "    \n",
    "    def save_model(self, n_epoch, save_path, loss, auc):\n",
    "        self.lastmodel = f\"{save_path}-e{n_epoch}-loss{loss:.3f}-auc{auc:.3f}.pth\"\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                \"best_valid_score\": self.best_valid_score,\n",
    "                \"n_epoch\": n_epoch,\n",
    "            },\n",
    "            self.lastmodel,\n",
    "        )\n",
    "        \n",
    "    def display_plots(self, mri_type):\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.title(\"{}: Training and Validation Loss\")\n",
    "        plt.plot(self.val_losses,label=\"val\")\n",
    "        plt.plot(self.train_losses,label=\"train\")\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.title(\"{}: Validation AUC-ROC\")\n",
    "        plt.plot(self.val_auc,label=\"val\")\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.ylabel(\"AUC\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    @staticmethod\n",
    "    def info_message(message, *args, end=\"\\n\"):\n",
    "        print(message.format(*args), end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a22aab-de07-474e-a973-7186a23f955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nontest_list, test_list = train_test_split(subIdx, \n",
    "                                          test_size=0.05,\n",
    "                                          stratify=lab.T,\n",
    "                                          random_state=SEED)\n",
    "nontest_label, test_label = train_test_split(lab.T, \n",
    "                                          test_size=0.05,\n",
    "                                          stratify=lab.T,\n",
    "                                          random_state=SEED)\n",
    "train_list, valid_list = train_test_split(nontest_list, \n",
    "                                          test_size=0.10,\n",
    "                                          stratify=nontest_label,\n",
    "                                          random_state=SEED)\n",
    "train_label, valid_label = train_test_split(nontest_label, \n",
    "                                          test_size=0.10,\n",
    "                                          stratify=nontest_label,\n",
    "                                          random_state=SEED);\n",
    "print('train case split: ',sum(train_label)[0],':',len(train_label)-sum(train_label)[0])\n",
    "print('valid case split: ',sum(valid_label)[0],':',len(valid_label)-sum(valid_label)[0])\n",
    "print('test case split: ',sum(test_label)[0],':',len(test_label)-sum(test_label)[0],'\\n')\n",
    "del nontest_list, nontest_label; # Save RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e4efa7-7905-4c0d-9bed-8765e914e2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# def build_model():\n",
    "# #     model = nets.ViTAutoEnc(in_channels=1, img_size=IM_SIZE, patch_size=PAT_SIZE)\n",
    "#     model = nets.DenseNet121(spatial_dims=3, in_channels=1, out_channels=1)\n",
    "#     return model  \n",
    "# class build_model():\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.net = EfficientNet3D.from_name(\"efficientnet-b0\", override_params={'num_classes': 2}, in_channels=1)\n",
    "#         n_features = self.net._fc.in_features\n",
    "#         self.net._fc = nn.Linear(in_features=n_features, out_features=1, bias=True)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         out = self.net(x)\n",
    "#         return out\n",
    "\n",
    "def train_mri_type(df_train, df_valid, train_lab, valid_lab, mri_type):\n",
    "#     print(df_train.shape, df_valid.shape)\n",
    "#     display(df_train.head())\n",
    "#     display(df_valid.head())\n",
    "\n",
    "    if mri_type == 'T1':\n",
    "        # Define transforms\n",
    "        train_transforms = Compose([\n",
    "        ScaleIntensity(minv=0.0, maxv=1.0),\n",
    "        AddChannel(),\n",
    "        Resize(T1_SIZE),\n",
    "        RandGaussianNoise(), \n",
    "        EnsureType(data_type='tensor')]);\n",
    "        train_ds = ImageDataset(image_files=T1w_images[df_train], labels=train_lab[:,0], transform=train_transforms);\n",
    "        model = Model(\n",
    "        image_size = T1_SIZE[0],\n",
    "        patch_size = T1_PATCH[0],\n",
    "        num_classes = 1,\n",
    "        dim = T1_SIZE[0]*4,\n",
    "        depth = 2,\n",
    "        heads = 16,\n",
    "        mlp_dim = T1_SIZE[0]*8,\n",
    "        channels = 1,\n",
    "        dropout = 0.1,\n",
    "        emb_dropout = 0.1\n",
    "        )\n",
    "    elif mri_type == 'EPI':\n",
    "        # Define transforms\n",
    "        train_transforms = Compose([EnsureType(data_type='tensor')]);\n",
    "        train_ds = CSVDataset(src=list(EPI_images[df_train]), labels=train_lab[:,0], transform=train_transforms);\n",
    "        model = Model(\n",
    "        image_size = EPI_SIZE[0],\n",
    "        patch_size = EPI_PAT[0],\n",
    "        num_classes = 1,\n",
    "        dim = EPI_SIZE[0]*4,\n",
    "        depth = 2,\n",
    "        heads = 16,\n",
    "        mlp_dim = EPI_SIZE[0]*8,\n",
    "        channels = 1,\n",
    "        dropout = 0.1,\n",
    "        emb_dropout = 0.1\n",
    "        )\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_of_cores, pin_memory=pin_memory);\n",
    "\n",
    "    # create a validation data loader\n",
    "    if mri_type == 'T1':\n",
    "        val_transforms = Compose([\n",
    "        ScaleIntensity(minv=0.0, maxv=1.0),\n",
    "        AddChannel(),\n",
    "        Resize(T1_SIZE),\n",
    "        EnsureType(data_type='tensor')]);\n",
    "        valid_ds = ImageDataset(image_files=T1w_images[df_valid], labels=valid_lab[:,0], transform=val_transforms);\n",
    "        model = Model(\n",
    "        image_size = T1_SIZE[0],\n",
    "        patch_size = T1_PATCH[0],\n",
    "        num_classes = 1,\n",
    "        dim = T1_SIZE[0]*4,\n",
    "        depth = 2,\n",
    "        heads = 16,\n",
    "        mlp_dim = T1_SIZE[0]*8,\n",
    "        channels = 1,\n",
    "        dropout = 0.1,\n",
    "        emb_dropout = 0.1\n",
    "        )\n",
    "    elif mri_type == 'EPI':\n",
    "        val_transforms = Compose([EnsureType(data_type='tensor')]);\n",
    "        valid_ds = CSVDataset(src=list(EPI_images[df_valid]), labels=valid_lab[:,0], transform=val_transforms);\n",
    "        model = Model(\n",
    "        image_size = EPI_SIZE[0],\n",
    "        patch_size = EPI_PAT[0],\n",
    "        num_classes = 1,\n",
    "        dim = EPI_SIZE[0]*4,\n",
    "        depth = 2,\n",
    "        heads = 16,\n",
    "        mlp_dim = EPI_SIZE[0]*8,\n",
    "        channels = 1,\n",
    "        dropout = 0.1,\n",
    "        emb_dropout = 0.1\n",
    "        )\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, num_workers=num_of_cores, pin_memory=pin_memory)\n",
    "\n",
    "#     model = build_model()\n",
    "   \n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    criterion = torch_functional.binary_cross_entropy_with_logits\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model, \n",
    "        device, \n",
    "        optimizer, \n",
    "        criterion\n",
    "    )\n",
    "\n",
    "    history = trainer.fit(\n",
    "        N_EPOCHS, \n",
    "        train_loader, \n",
    "        valid_loader, \n",
    "        f\"{mri_type}\", \n",
    "        N_EPOCHS,\n",
    "    )\n",
    "    \n",
    "    trainer.display_plots(mri_type)\n",
    "    \n",
    "    return trainer.lastmodel\n",
    "\n",
    "modelfiles = None\n",
    "\n",
    "if not modelfiles:\n",
    "    modelfiles = [train_mri_type(train_list, valid_list, train_label, valid_label, m) for m in mri_types]\n",
    "    print(modelfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b3201-f061-475d-8901-46bf8450c91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(modelfile, df, mri_type, split):\n",
    "    print(\"Predict:\", modelfile, mri_type, df.shape)\n",
    "    \n",
    "    # create a validation data loader\n",
    "    if mri_type == 'T1':\n",
    "        val_transforms = Compose([\n",
    "        ScaleIntensity(minv=0.0, maxv=1.0),\n",
    "        AddChannel(),\n",
    "        Resize(T1_SIZE),\n",
    "        EnsureType(data_type='tensor')]);\n",
    "        test_ds = ImageDataset(image_files=T1w_images[test_list], labels=test_label[:,0], transform=val_transforms)\n",
    "        model = Model(\n",
    "        image_size = T1_SIZE[0],\n",
    "        patch_size = T1_PATCH[0],\n",
    "        num_classes = 1,\n",
    "        dim = T1_SIZE[0]*4,\n",
    "        depth = 2,\n",
    "        heads = 16,\n",
    "        mlp_dim = T1_SIZE[0]*8,\n",
    "        channels = 1,\n",
    "        dropout = 0.1,\n",
    "        emb_dropout = 0.1\n",
    "        )\n",
    "    elif mri_type == 'EPI':\n",
    "        val_transforms = Compose([EnsureType(data_type='tensor')]);\n",
    "        test_ds = ImageDataset(src=list(EPI_images[test_list]), labels=test_label[:,0], transform=val_transforms)\n",
    "        model = Model(\n",
    "        image_size = EPI_SIZE[0],\n",
    "        patch_size = EPI_PAT[0],\n",
    "        num_classes = 1,\n",
    "        dim = EPI_SIZE[0]*4,\n",
    "        depth = 2,\n",
    "        heads = 16,\n",
    "        mlp_dim = EPI_SIZE[0]*8,\n",
    "        channels = 1,\n",
    "        dropout = 0.1,\n",
    "        emb_dropout = 0.1\n",
    "        )\n",
    "    test_loader = DataLoader(test_ds, batch_size=test_batch, num_workers=num_of_cores, pin_memory=pin_memory)\n",
    "   \n",
    "    model.to(device)\n",
    "    \n",
    "    checkpoint = torch.load(modelfile)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    y_pred = []\n",
    "    ids = []\n",
    "\n",
    "    for e, batch in enumerate(data_loader,1):\n",
    "        print(f\"{e}/{len(data_loader)}\", end=\"\\r\")\n",
    "        with torch.no_grad():\n",
    "            tmp_pred = torch.sigmoid(model(torch.tensor(batch[\"X\"]).float().to(device)).squeeze(1)).cpu().numpy().squeeze()\n",
    "            if tmp_pred.size == 1:\n",
    "                y_pred.append(tmp_pred)\n",
    "            else:\n",
    "                y_pred.extend(tmp_pred.tolist())\n",
    "            ids.extend(batch[\"id\"].numpy().tolist())\n",
    "            \n",
    "    preddf = pd.DataFrame({\"sub\": ids, \"pred\": y_pred}) \n",
    "    preddf = preddf.set_index(\"pred\")\n",
    "    return preddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950077cd-4da6-464d-af86-97adbfad3e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df_valid.set_index(\"sub\")\n",
    "df_pred[\"pred\"] = 0\n",
    "for m, mtype in zip(modelfiles,  mri_types):\n",
    "    pred = predict(m, df_pred, mtype, \"train\")\n",
    "    df_pred[\"pred\"] += pred[\"pred\"]\n",
    "df_pred[\"pred\"] /= len(modelfiles)\n",
    "auc = roc_auc_score(df_pred[\"pred\"], df_pred[\"pred\"])\n",
    "print(f\"Validation ensemble AUC: {auc:.4f}\")\n",
    "print(df_pred[\"pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faccb84-b481-4f5d-8f45-fb38c25181c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rearrange(img, 'b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1 = PAT_SIZE[0], p2 = PAT_SIZE[0], p3 = PAT_SIZE[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NGC-PyTorch-1.9",
   "language": "python",
   "name": "ngc-pytorch-1.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
