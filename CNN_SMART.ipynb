{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc3febc-4968-44dc-abd3-c92ce75f9cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"/home/alexanderalbizu\")\n",
    "sys.path.append(\"/home/alexanderalbizu/.local/bin\")\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"CNN.ipynb\"\n",
    "#!python setup.py develop \n",
    "# !pip install wandb\n",
    "# !pip install 'monai[all]'\n",
    "#!pip -q install vit_pytorch\n",
    "#!pip -q install linformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be71323-075d-4572-9b67-5e255af752de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.8.1+331.g6a301d51.dirty\n",
      "Numpy version: 1.20.1\n",
      "Pytorch version: 1.9.0+cu111\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "MONAI rev id: 6a301d51fbbb1803b7349a85c9bfa398f19ee0f9\n",
      "MONAI __file__: /home/alexanderalbizu/MONAI/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.8\n",
      "Nibabel version: 3.2.1\n",
      "scikit-image version: 0.19.1\n",
      "Pillow version: 9.1.1\n",
      "Tensorboard version: 1.15.0+nv\n",
      "gdown version: 4.4.0\n",
      "TorchVision version: 0.10.0+cu111\n",
      "tqdm version: 4.53.0\n",
      "lmdb version: 1.2.1\n",
      "psutil version: 5.8.0\n",
      "pandas version: 1.1.4\n",
      "einops version: 0.4.1\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: 1.26.1\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maalbizu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2020 MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import glob, math, os, shutil, tempfile, time, monai, torch, random\n",
    "\n",
    "import wandb as wb\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from monai.networks.utils import eval_mode\n",
    "from monai.config import print_config\n",
    "from monai.data import (\n",
    "    CacheDataset,\n",
    "    DataLoader,\n",
    "    ThreadDataLoader,\n",
    "    ImageDataset,\n",
    "    Dataset,\n",
    "    decollate_batch,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "# from vit_pytorch.efficient import ViT\n",
    "# from linformer import Linformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.losses import DiceLoss, DiceCELoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.layers import Act, Norm\n",
    "from monai.transforms import (\n",
    "    AddChannel,\n",
    "    Compose,\n",
    "    RandGaussianNoise,\n",
    "    Resize,\n",
    "    AsChannelFirst,\n",
    "    RemoveRepeatedChannel,\n",
    "    Orientation,\n",
    "    RandRotate90,\n",
    "    RandBiasField,\n",
    "    ScaleIntensity,\n",
    "    ToDevice,\n",
    "    EnsureType,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print_config()\n",
    "wb.login(); # 7e5f63e5846f29b034d98806712ab047df76834d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b04f482-1382-49f6-a7e7-c59f81f42dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alexanderalbizu/MONAI/wandb/run-20220708_113554-evgyl67r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/aalbizu/SMART-CNN/runs/evgyl67r\" target=\"_blank\">dazzling-mountain-25</a></strong> to <a href=\"https://wandb.ai/aalbizu/SMART-CNN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Fold  1\n",
      "test subs:  [9051 9023]\n",
      "train case split:  6 : 4\n",
      "valid case split:  1 : 1\n",
      "test case split:  0 : 2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         167.79079236]\n",
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         167.79079236]\n",
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         167.79079236]\n",
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         167.79079236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 96, 96, 96]) tensor([[1, 0]]) torch.Size([1, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         167.79079236]\n",
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         167.79079236]\n",
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         167.79079236]\n",
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         167.79079236]\n",
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         167.79079236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 - train_loss : 1.1418 - train_acc: 0.2500 - val_loss : 38.3673 - val_acc: 0.5000\n",
      "\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         167.79079236]\n",
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         167.79079236]\n",
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         167.79079236]\n",
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         167.79079236]\n",
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         167.79079236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 - train_loss : 1.3713 - train_acc: 0.3333 - val_loss : 13.1579 - val_acc: 0.5000\n",
      "\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying image pixdim from [1. 1. 1. 1.] to [  1.           1.           1.         167.79079236]\n"
     ]
    }
   ],
   "source": [
    "# Set data directory\n",
    "rootDir = '/blue/camctrp/working/Alejandro/StimBrain/'\n",
    "batch_size=3\n",
    "im_size = (96,96,96)\n",
    "# pat_size = (24,24,24)\n",
    "grad_clip = 2\n",
    "step_size = 1\n",
    "gamma = 7e-1\n",
    "seed = 42\n",
    "lr = 1e-2\n",
    "wd = 0\n",
    "K = 7 # MUST BE A FACTOR OF NUM SUBS\n",
    "val_interval = 1\n",
    "epoch_loss_values = [] # Pre-Allocate\n",
    "epoch_acc_values = [] # Pre-Allocate\n",
    "max_epochs = 50\n",
    "subs = np.array([9009, 9015, 9022, 9040, 9044, 9045, 9051, 9021, 9023, 9031, 9032, 9047, 9048, 9054]) # Subjects\n",
    "beh = np.array([32, 26, 19, 20, 20, 17, 15, 2, 4, 6, 10, 16, 18, 7]) # Targeted Behavior Change\n",
    "num_cores = int(os.environ[\"SLURM_CPUS_PER_TASK\"]);\n",
    "\n",
    "# Define Group Labels as above/below Median\n",
    "lab = np.int64(np.zeros([len(beh),1]));\n",
    "lab[beh >= np.median(beh)] = 1; # Responders\n",
    "lab[beh < np.median(beh)] = 0; # Non Responders\n",
    "\n",
    "wb.init(project=\"SMART-CNN\",\n",
    "        config={\n",
    "       \"batch_size\": batch_size,\n",
    "       \"n_channels\": 3,\n",
    "       \"n_epoch\": max_epochs,\n",
    "       \"image_size\": im_size,\n",
    "       \"folds\": K,\n",
    "       \"gradient_clip\": grad_clip,\n",
    "       \"learning_rate\": lr,\n",
    "       \"step_size\": step_size,\n",
    "       \"network\": \"ResNet10\",\n",
    "       \"gamma\": gamma,\n",
    "       \"weight_decay\": wd,\n",
    "       \"dataset\": \"SB\",\n",
    "        })\n",
    "run_id = wb.run.name;\n",
    "    \n",
    "class Diagnosis(Enum):\n",
    "    NonResponder = 0\n",
    "    Responder = 1\n",
    "    \n",
    "images = np.array([os.path.join(rootDir,''.join(['FS6.0_sub-',str(subs[s]),'_ses01']),'T1_xyzJbrain.nii') for s in range(subs.shape[0])]);\n",
    "\n",
    "# Plot Labels\n",
    "# plt.imshow(lab); plt.axis('off'); plt.title('Class Labels');\n",
    "\n",
    "# def seed_everything(seed):\n",
    "#     random.seed(seed)\n",
    "#     os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "# seed_everything(seed)    \n",
    "\n",
    "def save_model(n_epoch, save_path, run_id, f):\n",
    "    lastmodel = f\"{save_path}-{run_id}-fold{f}.pth\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"best_valid_score\": best_valid_score,\n",
    "            \"n_epoch\": n_epoch,\n",
    "        },\n",
    "        lastmodel,\n",
    "    )\n",
    "\n",
    "butt = [0,1];\n",
    "testIdx = np.array(random.sample(list(np.arange(subs.shape[0])),subs.shape[0]));\n",
    "alltargs = []; allpreds = []; allscores = []; # Pre-Allocate\n",
    "for f in range(K):\n",
    "    print(\"Beginning Fold \", f+1)\n",
    "    test_subs = subs[testIdx[butt]]; print('test subs: ', test_subs)\n",
    "    test_list = images[testIdx[butt]]; \n",
    "    test_label = lab[testIdx[butt]];\n",
    "    nontest_list = images[np.logical_not(np.isin(subs,test_subs))]\n",
    "    y_nontest = lab[np.logical_not(np.isin(subs,test_subs))]\n",
    "    train_list, valid_list, train_label, valid_label = train_test_split(nontest_list, y_nontest, test_size=0.10, stratify=y_nontest, random_state=seed);\n",
    "    \n",
    "    train_list = np.array(train_list); train_label = np.array(train_label);                  \n",
    "    valid_list = np.array(valid_list); valid_label = np.array(valid_label);                  \n",
    "    test_list = np.array(test_list); test_label = np.array(test_label);\n",
    "    \n",
    "# print(images.shape, lab.shape)\n",
    "# nontest_list, test_list = train_test_split(images, \n",
    "#                                           test_size=0.1,\n",
    "#                                           stratify=lab.T,\n",
    "#                                           random_state=seed)\n",
    "# nontest_label, test_label = train_test_split(lab.T, \n",
    "#                                           test_size=0.1,\n",
    "#                                           stratify=lab.T,\n",
    "#                                           random_state=seed)\n",
    "# train_list, valid_list = train_test_split(nontest_list, \n",
    "#                                           test_size=0.3,\n",
    "#                                           stratify=nontest_label,\n",
    "#                                           random_state=seed)\n",
    "# train_label, valid_label = train_test_split(nontest_label, \n",
    "#                                           test_size=0.3,\n",
    "#                                           stratify=nontest_label,\n",
    "#                                           random_state=seed)\n",
    "\n",
    "# Oversampling the Unbalnaced Class\n",
    "# train_list = np.array(np.hstack([[train_list.T],[train_list.T]]))[0,:];\n",
    "# train_label = np.array([np.int64(np.hstack([train_label[:,0],train_label[:,0]]))]).T;\n",
    "\n",
    "# print(train_list.shape,valid_list.shape,test_list.shape)\n",
    "# print(train_label.shape,valid_label.shape,test_label.shape)\n",
    "\n",
    "    print('train case split: ',sum(train_label)[0],':',len(train_label)-sum(train_label)[0])\n",
    "    print('valid case split: ',sum(valid_label)[0],':',len(valid_label)-sum(valid_label)[0])\n",
    "    print('test case split: ',sum(test_label)[0],':',len(test_label)-sum(test_label)[0],'\\n')\n",
    "    del nontest_list, y_nontest; # Save RAM\n",
    "\n",
    "# Plot Responder Mean\n",
    "# plt.rcParams['figure.figsize'] = [5,5];\n",
    "# print(images[0])\n",
    "# nii = nib.load(images[0]); print(nii.shape) # Load Each Electrode\n",
    "# plt.imshow(nii.get_fdata()[:, :, 120,0].T, cmap=\"turbo\", origin=\"lower\"); plt.axis('off'); # Plot\n",
    "\n",
    "# Represent labels in one-hot format for binary classifier training,\n",
    "# BCEWithLogitsLoss requires target to have same shape as input\n",
    "#     labels = torch.nn.functional.one_hot(torch.as_tensor(lab.T)).long();\n",
    "    train_lab = torch.nn.functional.one_hot(torch.as_tensor(train_label.T)).long(); \n",
    "    valid_lab = torch.nn.functional.one_hot(torch.as_tensor(valid_label.T)).long();\n",
    "    test_lab = torch.nn.functional.one_hot(torch.as_tensor(test_label.T)).long();\n",
    "\n",
    "    # Define transforms\n",
    "    train_transforms = Compose([\n",
    "    #     ScaleIntensity(minv=0.0, maxv=1.0),\n",
    "        AsChannelFirst(channel_dim=3),\n",
    "    #     AddChannel(),\n",
    "        Resize(im_size),\n",
    "    #     RandBiasField(),\n",
    "    #     RandGaussianNoise(prob=0.1), \n",
    "        EnsureType(data_type='tensor')]);\n",
    "\n",
    "    val_transforms = Compose([\n",
    "    #     ScaleIntensity(minv=0.0, maxv=1.0),\n",
    "        AsChannelFirst(channel_dim=3),\n",
    "    #     AddChannel(),\n",
    "        Resize(im_size),\n",
    "        EnsureType(data_type='tensor')]);\n",
    "\n",
    "    # Define nifti dataset, data loader\n",
    "    check_ds = ImageDataset(image_files=train_list, labels=train_lab[0,:,:], transform=train_transforms);\n",
    "    check_loader = DataLoader(check_ds, batch_size=1, num_workers=num_cores, pin_memory=pin_memory); \n",
    "\n",
    "    im, label = monai.utils.misc.first(check_loader); print(im.shape, label, label.shape)\n",
    "    plt.imshow(im[0,0,:,:,60].T, cmap=\"turbo\", origin=\"lower\"); plt.axis('off'); del check_ds, check_loader, im, label;\n",
    "\n",
    "    # create a training data loader\n",
    "    train_ds = ImageDataset(image_files=train_list, labels=train_lab[0,:,:], transform=train_transforms);\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_cores, pin_memory=pin_memory);\n",
    "\n",
    "    # create a validation data loader\n",
    "    val_ds = ImageDataset(image_files=valid_list, labels=valid_lab[0,:,:], transform=val_transforms);\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=num_cores, pin_memory=pin_memory)\n",
    "\n",
    "    # Create DenseNet121, CrossEntropyLoss and Adam optimizer\n",
    "    # model = monai.networks.nets.DenseNet121(spatial_dims=3, in_channels=3, out_channels=2).to(device)\n",
    "    model = monai.networks.nets.resnet10(spatial_dims=3, num_classes=2).to(device)\n",
    "\n",
    "    # Loss Function\n",
    "    loss_fx = torch.nn.BCEWithLogitsLoss() \n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    wb.watch(model, log='all')\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    best_valid_score = 99999; # Initialize Loss\n",
    "    lastmodel = None\n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "\n",
    "        for batch_data in train_loader:\n",
    "            inputs, label = batch_data[0].to(device), batch_data[1].to(device);\n",
    "\n",
    "            # Evaluate Model\n",
    "            output = model(inputs);\n",
    "            loss = loss_fx(output, label.float())\n",
    "\n",
    "            # Update Gradient\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # Evaluate Model\n",
    "                output = model(inputs); epoch_prob = torch.nn.functional.softmax(output,dim=1)\n",
    "                loss = loss_fx(output, label.float())\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Accuracy\n",
    "            acc = (torch.nn.functional.softmax(epoch_prob,dim=1).argmax(dim=1) == label.argmax(dim=1)).float().mean()\n",
    "            epoch_accuracy += acc / len(train_loader)\n",
    "            epoch_acc_values.append(acc)\n",
    "\n",
    "            # Loss\n",
    "            epoch_loss += loss / len(train_loader)\n",
    "            epoch_loss_values.append(epoch_loss)\n",
    "    #         print(f\"train loss: {loss.item():.4f}\")\n",
    "            wb.log({'train_loss': loss, 'train_acc': acc})\n",
    "\n",
    "        if epoch % val_interval == 0: # Validation Interval\n",
    "            with eval_mode(model):\n",
    "                epoch_val_accuracy = 0; epoch_val_loss = 0;\n",
    "                for val_data in val_loader:\n",
    "                    val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "\n",
    "                    val_output = model(val_images); val_prob = torch.nn.functional.softmax(val_output,dim=1);\n",
    "                    val_loss = loss_fx(val_output, val_labels.float())\n",
    "\n",
    "                    val_acc = (val_prob.argmax(dim=1) == val_labels.argmax(dim=1)).float().mean()\n",
    "                    epoch_val_accuracy += val_acc / len(val_loader)\n",
    "                    epoch_val_loss += val_loss / len(val_loader)\n",
    "                    wb.log({'val_loss': val_loss, 'val_acc': val_acc})\n",
    "            print(\n",
    "                f\"Epoch : {epoch+1} - train_loss : {epoch_loss:.4f} - train_acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "            )\n",
    "\n",
    "            # Save Best Model\n",
    "            if best_valid_score > epoch_val_loss:\n",
    "                print(\"model saved\")\n",
    "                save_model(epoch, \"T1\", run_id,f)\n",
    "                best_valid_score = epoch_val_loss\n",
    "\n",
    "    #     else:    \n",
    "    #         print(\n",
    "    #             f\"Epoch : {epoch+1} - train_loss : {epoch_loss:.4f} - train_acc: {epoch_accuracy:.4f} \\n\"\n",
    "    #         )\n",
    "\n",
    "#     wb.run.log_code(root=os.path.join(os.getcwd(),\"CNN_SMART.ipynb\"));\n",
    "    \n",
    "    ##############\n",
    "    # Test Model #\n",
    "    ##############\n",
    "    \n",
    "    modelfile = f'T1-'+run_id+'.pth'; # HARDCODED\n",
    "    checkpoint = torch.load(modelfile)\n",
    "    print(f\"Loading {modelfile} ...\")\n",
    "\n",
    "    # create a validation data loader\n",
    "    test_ds = ImageDataset(image_files=test_list, labels=test_lab[0,:,:], transform=val_transforms)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, num_workers=num_cores, pin_memory=pin_memory)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "    with eval_mode(model):\n",
    "        y_pred = torch.tensor([], dtype=torch.float32, device=device)\n",
    "        y = torch.tensor([], dtype=torch.long, device=device)\n",
    "        test_acc = 0\n",
    "        for test_data in test_loader:\n",
    "            test_images, test_labels = test_data[0].to(\n",
    "                device), test_data[1].to(device)\n",
    "\n",
    "            outputs = model(test_images); probs = torch.nn.functional.softmax(outputs,dim=1);\n",
    "            test_acc += (probs.argmax(dim=1) == test_labels.argmax(dim=1)).float().mean();\n",
    "            y_pred = torch.cat([y_pred, probs.argmax(dim=1)], dim=0)\n",
    "            y = torch.cat([y, test_labels.argmax(dim=1)], dim=0);\n",
    "        test_acc = test_acc / len(test_loader)\n",
    "        alltargs.append(y.cpu().numpy())\n",
    "        allpreds.append(y_pred.cpu().numpy())\n",
    "        allscores.append(y_prob.cpu().numpy())\n",
    "        print(f\"Fold {f+1} Test Accuracy: \", test_acc.cpu().numpy())\n",
    "        wb.log({'test_acc': test_acc})\n",
    "    butt = [ x + 2 for x in butt ]\n",
    "        \n",
    "print(classification_report(\n",
    "    alltargs,\n",
    "    allpreds,\n",
    "    target_names=[d.name for d in Diagnosis]))\n",
    "\n",
    "cm = confusion_matrix(\n",
    "    alltargs,\n",
    "    allpreds,\n",
    "    normalize='true')\n",
    "\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm,\n",
    "    display_labels=[d.name for d in Diagnosis])\n",
    "disp.plot(ax=plt.subplots(1, 1, facecolor='white')[1])\n",
    "\n",
    "wb.log({\"conf_mat\" : wb.plot.confusion_matrix(probs=None,\n",
    "                        y_true=alltargs, preds=allpreds,\n",
    "                        class_names=Diagnosis)})\n",
    "wb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9c1e16-bda4-44b8-86a6-bd3b12459ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cam = monai.visualize.CAM(nn_module=model_3d, target_layers=\"class_layers.relu\", fc_layers=\"class_layers.out\")\n",
    "cam = monai.visualize.GradCAMpp(\n",
    "    nn_module=model, target_layers=\"class_layers.relu\"\n",
    ")\n",
    "# cam = monai.visualize.GradCAMpp(nn_module=model_3d, target_layers=\"class_layers.relu\")\n",
    "print(\n",
    "    \"original feature shape\",\n",
    "    cam.feature_map_size([1, 1] + list(im_size), device),\n",
    ")\n",
    "print(\"upsampled feature shape\", [1, 1] + list(im_size))\n",
    "\n",
    "occ_sens = monai.visualize.OcclusionSensitivity(\n",
    "    nn_module=model, mask_size=12, n_batch=1, stride=24\n",
    ")\n",
    "\n",
    "# For occlusion sensitivity, inference must be run many times. Hence, we can use a\n",
    "# bounding box to limit it to a 2D plane of interest (z=the_slice) where each of\n",
    "# the arguments are the min and max for each of the dimensions (in this case CHWD).\n",
    "the_slice = train_ds[0][0].shape[-1] // 2\n",
    "occ_sens_b_box = [-1, -1, -1, -1, -1, -1, the_slice, the_slice]\n",
    "\n",
    "train_transforms.set_random_state(42)\n",
    "n_examples = 3\n",
    "subplot_shape = [3, n_examples]\n",
    "fig, axes = plt.subplots(*subplot_shape, figsize=(25, 15), facecolor=\"white\")\n",
    "items = np.random.choice(len(train_ds), size=len(train_ds), replace=False)\n",
    "\n",
    "example = 0\n",
    "for item in items:\n",
    "\n",
    "    data = train_ds[\n",
    "        item\n",
    "    ]  # this fetches training data with random augmentations\n",
    "    image, label = data[0].to(device).unsqueeze(0), data[1][1]\n",
    "    y_pred = model(image); prob = torch.nn.functional.softmax(y_pred,dim=1);\n",
    "    pred_label = prob.argmax(dim=1);\n",
    "    \n",
    "    # Only display preMCI\n",
    "    if np.not_equal(label,1):\n",
    "        continue\n",
    "\n",
    "    img = image.detach().cpu().numpy()[..., the_slice]\n",
    "\n",
    "    name = \"actual: \"\n",
    "    name += \"Responder\" if np.equal(label,1) else \"Non-Responder\"\n",
    "    name += \"\\npred: \"\n",
    "    name += \"Responder\" if np.equal(pred_label.cpu().numpy(),1) else \"Non-Responder\"\n",
    "    name += f\"\\nResponder: {prob[0,1]:.3}\"\n",
    "    name += f\"\\nNon-Responder: {prob[0,0]:.3}\"\n",
    "\n",
    "    # run CAM\n",
    "    cam_result = cam(x=image, class_idx=None)\n",
    "    cam_result = cam_result[..., the_slice]\n",
    "\n",
    "    # run occlusion\n",
    "    occ_result, _ = occ_sens(x=image, b_box=occ_sens_b_box)\n",
    "    occ_result = occ_result[..., pred_label];\n",
    "\n",
    "    for row, (im, title) in enumerate(\n",
    "        zip(\n",
    "            [img, cam_result, occ_result],\n",
    "            [name, \"CAM\", \"Occ. Sens.\"],\n",
    "        )\n",
    "    ):\n",
    "        cmap = \"gray\" if np.equal(row,0) else \"jet\"\n",
    "        ax = axes[row, example]\n",
    "        if isinstance(im, torch.Tensor):\n",
    "            im = im.cpu().detach()\n",
    "        \n",
    "        im_show = ax.imshow(np.squeeze(im[0][0].T), cmap=cmap, origin='lower')\n",
    "\n",
    "        ax.set_title(title, fontsize=25)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    example += 1\n",
    "    if example == n_examples:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b34187-2846-4af6-b465-e1287152815e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NGC-PyTorch-1.9",
   "language": "python",
   "name": "ngc-pytorch-1.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
